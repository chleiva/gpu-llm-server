# gpu-llm-server
A lightweight FastAPI server for serving open-source LLMs with GPU acceleration and optional quantization. Simple, containerized inference API for offline local production deployments.
